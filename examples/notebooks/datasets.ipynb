{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import gym\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import yaml\n",
    "import hydra\n",
    "import myosuite\n",
    "import omegaconf\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a000d6",
   "metadata": {},
   "source": [
    "## Add task_id to bidex tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from robomimic_data import add_task_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Path(\"/home/krishnans/ngc/bidex_datasets\").rglob(\"*.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f43353",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = glob.glob('/home/krishnans/ngc/bidex_datasets/**/rollout*0.hdf5')\n",
    "task_names = [Path(data_path).parent.name for data_path in data_paths]\n",
    "print(task_names)\n",
    "task_set = 'bidex'\n",
    "\n",
    "add_task_id(data_paths, task_names, task_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a70204",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "switch the image channels to (c, h, w) if it is in the order of (h, w, c)\n",
    "'''\n",
    "def convert_hwc_to_chw(data):\n",
    "    if len(data.shape) >= 3 and data.shape[-1] < 5:\n",
    "        if type(data) == torch.tensor:\n",
    "            data = torch.moveaxis(data, -1, -3).contiguous()\n",
    "        elif type(data) == np.ndarray:\n",
    "            data = np.ascontiguousarray(np.moveaxis(data, -1, -3))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_articulate_to_robosuite(dataset_path, config_path=None):\n",
    "    config = {}\n",
    "    if config_path is not None:\n",
    "        config = yaml.safe_load(open(config_path, \"r\"))\n",
    "\n",
    "    with h5py.File(dataset_path, mode=\"a\") as data:\n",
    "        data.attrs[\"env_args\"] = json.dumps(config)\n",
    "        for ep in data[\"data\"].keys():\n",
    "            data[\"data/{}\".format(ep)].attrs[\"num_samples\"] = data[\"data/{}\".format(ep)][\"actions\"].shape[0]\n",
    "        # Create next_obs group for every demo in data, and copy obs/{key}.data[1:] to this group\n",
    "        for demo_key in data[\"data\"].keys():\n",
    "            demo_group = data[\"data\"][demo_key]\n",
    "            next_obs_group = demo_group.create_group(\"next_obs\")\n",
    "            \n",
    "            for obs_key in demo_group[\"obs\"].keys():\n",
    "                obs_data = demo_group[\"obs\"][obs_key]\n",
    "                next_obs_group.create_dataset(obs_key, data=obs_data[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import robomimic.utils.file_utils as FileUtils\n",
    "\n",
    "# the dataset registry can be found at robomimic/__init__.py\n",
    "from robomimic import DATASET_REGISTRY\n",
    "\n",
    "# set download folder and make it\n",
    "download_folder = \"/tmp/robomimic_ds_example\"\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "# download the dataset\n",
    "task = \"lift\"\n",
    "dataset_type = \"ph\"\n",
    "hdf5_type = \"low_dim\"\n",
    "FileUtils.download_url(\n",
    "    url=DATASET_REGISTRY[task][dataset_type][hdf5_type][\"url\"], \n",
    "    download_dir=download_folder,\n",
    ")\n",
    "\n",
    "# enforce that the dataset exists\n",
    "dataset_path = os.path.join(download_folder, \"low_dim_v141.hdf5\")\n",
    "assert os.path.exists(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bdec82",
   "metadata": {},
   "source": [
    "## Read quantities from dataset\n",
    "\n",
    "Next, let's demonstrate how to read different quantities from the dataset. There are scripts such as `scripts/get_dataset_info.py` that can help you easily understand the contents of a dataset, but in this example, we'll break down how to do this directly.\n",
    "\n",
    "First, let's take a look at the number of demonstrations in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a323e6",
   "metadata": {},
   "source": [
    "```python\n",
    "dataset_path = \"/home/krishnans/ngc/policy_learning_toolkit/datasets/articulate_multi_spray_scissors/train/merged.hdf5\"\n",
    "dataset_path = \"/home/krishnans/ngc/DexterousHands/policy_learning_toolkit/train/bidex_runs/ShadowHandScissors/ppo/ppo_seed-1/rollouts_1000.hdf5\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Myosuite task dataset paths\n",
    "dataset_root = Path(\"/juno/u/ksrini/multi_task_experts/datasets/myosuite\")\n",
    "dataset_path = list(dataset_root.iterdir())[0].absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cd8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file\n",
    "f = h5py.File(dataset_path, \"r\")\n",
    "\n",
    "# each demonstration is a group under \"data\"\n",
    "demos = list(f[\"data\"].keys())\n",
    "num_demos = len(demos)\n",
    "\n",
    "print(\"hdf5 file {} has {} demonstrations\".format(dataset_path, num_demos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ab260",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb073a0",
   "metadata": {},
   "source": [
    "Next, let's list all of the demonstrations, along with the number of state-action pairs in each demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda3e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each demonstration is named \"demo_#\" where # is a number.\n",
    "# Let's put the demonstration list in increasing episode order\n",
    "inds = np.argsort([int(elem[5:]) for elem in demos])\n",
    "demos = [demos[i] for i in inds]\n",
    "\n",
    "for ep in demos:\n",
    "    num_actions = f[\"data/{}/actions\".format(ep)].shape[0]\n",
    "    print(\"{} has {} samples\".format(ep, num_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff998d62",
   "metadata": {},
   "source": [
    "Now, let's dig into a single trajectory to take a look at some of the quantities in each demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7b497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at first demonstration\n",
    "demo_key = demos[0]\n",
    "demo_grp = f[\"data/{}\".format(demo_key)]\n",
    "\n",
    "# Each observation is a dictionary that maps modalities to numpy arrays, and\n",
    "# each action is a numpy array. Let's print the observations and actions for the \n",
    "# first 5 timesteps of this trajectory.\n",
    "for t in range(5):\n",
    "    print(\"timestep {}\".format(t))\n",
    "    obs_t = dict()\n",
    "    # each observation modality is stored as a subgroup\n",
    "    for k in demo_grp[\"obs\"]:\n",
    "        obs_t[k] = demo_grp[\"obs/{}\".format(k)][t] # numpy array\n",
    "    act_t = demo_grp[\"action\"][t]\n",
    "    \n",
    "    # pretty-print observation and action using json\n",
    "    obs_t_pp = { k : obs_t[k].tolist() for k in obs_t }\n",
    "    print(\"obs\")\n",
    "    print(json.dumps(obs_t_pp, indent=4))\n",
    "    print(\"action\")\n",
    "    print(act_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552be387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also grab multiple timesteps at once directly, or even the full trajectory at once\n",
    "first_ten_actions = demo_grp[\"actions\"][:10]\n",
    "print(\"shape of first ten actions {}\".format(first_ten_actions.shape))\n",
    "all_actions = demo_grp[\"actions\"][:]\n",
    "print(\"shape of all actions {}\".format(all_actions.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e59b01",
   "metadata": {},
   "source": [
    "# Dataset helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_hdf5_files(source_files, target_file):\n",
    "    \"\"\"\n",
    "    Merge multiple HDF5 files into a single file, ensuring that \"data/demo_X\" keys\n",
    "    are unique across the merged file.\n",
    "\n",
    "    Args:\n",
    "        source_files (list of str): List of paths to source HDF5 files to merge.\n",
    "        target_file (str): Path to the target HDF5 file to create.\n",
    "    \"\"\"\n",
    "    with h5py.File(target_file, 'w') as target_h5:\n",
    "        if \"data\" not in target_h5.keys():\n",
    "            target_h5.create_group(\"data\")  # Ensure the 'data' group exists in the target file\n",
    "        for source_file in source_files:\n",
    "            with h5py.File(source_file, 'r') as source_h5:\n",
    "                # Iterate over each \"demo_X\" group in the source file\n",
    "                for key in source_h5['data'].keys():\n",
    "                    # Extract the demo number from the key and increment it by the current offset\n",
    "                    new_demo_key = f'demo_{len(target_h5[\"data\"].keys())}'\n",
    "\n",
    "                    # Copy the group to the new file with the updated demo key\n",
    "                    source_h5.copy(f'data/{key}', target_h5[\"data\"], new_demo_key)\n",
    "\n",
    "\n",
    "def convert_camera_obs_hwc(source_file, target_file):\n",
    "    \"\"\"\n",
    "    Convert all observations with the name \"camera\" in the key from (c, h, w) to (h, w, c) shape ordering,\n",
    "    and change dtype to np.uint8 for a single source file, and map it to a target file.\n",
    "\n",
    "    Args:\n",
    "        source_file (str): Path to the source HDF5 file.\n",
    "        target_file (str): Path to the target HDF5 file to create.\n",
    "    \"\"\"\n",
    "    with h5py.File(source_file, 'r') as source_h5:\n",
    "        with h5py.File(target_file, 'w') as target_h5:\n",
    "            # Iterate over each \"demo_X\" group in the source file\n",
    "            for demo_key in source_h5['data'].keys():\n",
    "                demo_group = source_h5['data'][demo_key]\n",
    "                target_demo_group = target_h5.require_group(f'data/{demo_key}')\n",
    "                \n",
    "                # Check if the \"obs\" group exists in the demo group\n",
    "                for key in demo_group:\n",
    "                    if key == \"obs\":\n",
    "                        obs_group = demo_group[\"obs\"]\n",
    "                        # Add \"obs\" group to the target_demo_group if it doesn't already exist\n",
    "                        target_demo_group.create_group(\"obs\")\n",
    "                        # Iterate over each item in the \"obs\" group\n",
    "                        for item_key in obs_group.keys():\n",
    "                            item = obs_group[item_key]\n",
    "                            \n",
    "                            # Check if the item is an observation with \"camera\" in the key\n",
    "                            if \"camera\" in item_key and len(item.shape) == 4:  # Assuming shape is (timesteps, c, h, w)\n",
    "                                # Convert (c, h, w) to (h, w, c) and change dtype to np.uint8\n",
    "                                converted_item = item[:].transpose(0, 2, 3, 1).astype(np.uint8)\n",
    "                                target_demo_group[\"obs\"].create_dataset(item_key, data=converted_item)\n",
    "                            else:\n",
    "                                # For other items within \"obs\", just copy them as they are\n",
    "                                obs_group.copy(item_key, target_demo_group[\"obs\"])\n",
    "                    else:\n",
    "                        demo_group.copy(f'{key}', target_demo_group, key)\n",
    "                        # target_demo_group.create_dataset(key, data=demo_group[key])\n",
    "\n",
    "\n",
    "def convert_articulate_to_robosuite(dataset_path, config_path=None):\n",
    "    config = {}\n",
    "    if config_path is not None:\n",
    "        config = yaml.safe_load(open(config_path, \"r\"))\n",
    "\n",
    "    with h5py.File(dataset_path, mode=\"a\") as data:\n",
    "        data.attrs[\"env_args\"] = json.dumps(config)\n",
    "        for ep in data[\"data\"].keys():\n",
    "            data[\"data/{}\".format(ep)].attrs[\"num_samples\"] = data[\"data/{}\".format(ep)][\"actions\"].shape[0]\n",
    "\n",
    "MYOSUITE_TASKS = {\n",
    "\t'myo-reach': 'myoHandReachFixed-v0',\n",
    "\t'myo-reach-hard': 'myoHandReachRandom-v0',\n",
    "\t'myo-pose': 'myoHandPoseFixed-v0',\n",
    "\t'myo-pose-hard': 'myoHandPoseRandom-v0',\n",
    "\t'myo-obj-hold': 'myoHandObjHoldFixed-v0',\n",
    "\t'myo-obj-hold-hard': 'myoHandObjHoldRandom-v0',\n",
    "\t'myo-key-turn': 'myoHandKeyTurnFixed-v0',\n",
    "\t'myo-key-turn-hard': 'myoHandKeyTurnRandom-v0',\n",
    "\t'myo-pen-twirl': 'myoHandPenTwirlFixed-v0',\n",
    "\t'myo-pen-twirl-hard': 'myoHandPenTwirlRandom-v0',\n",
    "}\n",
    "\n",
    "def add_mask(dataset_paths):\n",
    "    \"\"\"Adds a mask to the dataset to indicate which demonstrations are successful.\n",
    "    A demonstration is successful if it has at least one sample.\"\"\"\n",
    "    for dataset_path in dataset_paths:\n",
    "        cfg = omegaconf.OmegaConf.load(dataset_paths[0].parent/'.hydra/config.yaml')\n",
    "        min_ep_len = gym.envs.registry.spec(MYOSUITE_TASKS[cfg.task]).max_episode_steps + 1\n",
    "        with h5py.File(dataset_path, \"a\") as data:\n",
    "            success_demos = []\n",
    "            for ep in data[\"data\"].keys():\n",
    "                        if data[f\"data/{ep}\"].attrs['num_samples'] >= 1:\n",
    "                            success_demos.append(ep.encode('utf-8'))  # HDF5 requires bytes for strings\n",
    "            if \"mask\" not in data.keys():\n",
    "                data.create_group(\"mask\")\n",
    "            if \"traj_success\" not in data[\"mask\"].keys():\n",
    "                data[\"mask\"].create_dataset(\"traj_success\", data=np.array(success_demos, dtype=h5py.string_dtype(encoding='utf-8')))\n",
    "            else:\n",
    "                print(f\"skipping mask for {dataset_path}, traj_success already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"/juno/u/ksrini/multi_task_experts/bidex_datasets/datasets/articulate_multi_expert_spray_bottle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52621150",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files = list(map(str, sorted(dataset_path.rglob(\"*.hdf5\"), key=lambda x: int(x.name.split('.')[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad47c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"/juno/u/ksrini/multi_task_experts/allegro_datasets/datasets\")\n",
    "source_files = list(map(str, sorted(dataset_path.rglob(\"*.hdf5\"), key=lambda x: int(x.name.split('.')[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215585ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_source_files = []\n",
    "for file in source_files:\n",
    "    try:\n",
    "        with h5py.File(file, \"r\"):\n",
    "            valid_source_files.append(file)\n",
    "    except OSError:\n",
    "        print(f\"Skipping {file}, unable to open with h5py.File due to OSError\")\n",
    "\n",
    "source_files = valid_source_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_hdf5_files(source_files, str(dataset_path / \"merged.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2914f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(source_files[0], \"r\") as f:\n",
    "    print(f[f'data/demo_0/obs/'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e598bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = source_files[0]\n",
    "\n",
    "with h5py.File(source_file, \"r\") as f:\n",
    "    hand_camera_image = f[f'data/demo_0/obs/hand_camera'][0].astype(int)\n",
    "    plt.imshow(hand_camera_image.transpose(1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cf857",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file = dataset_path.parent / \"transformed.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(target_file, 'w') as target_h5:\n",
    "    if \"data\" not in target_h5.keys():\n",
    "        target_h5.create_group(\"data\")  # Ensure the 'data' group exists in the target file\n",
    "    for source_file in source_files:\n",
    "        with h5py.File(source_file, 'r') as source_h5:\n",
    "            # Iterate over each \"demo_X\" group in the source file\n",
    "            for key in source_h5['data'].keys():\n",
    "                # Extract the demo number from the key and increment it by the current offset\n",
    "                new_demo_key = f'demo_{len(target_h5[\"data\"].keys())}'\n",
    "\n",
    "                # Copy the group to the new file with the updated demo key\n",
    "                source_h5.copy(f'data/{key}', target_h5[\"data\"], new_demo_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84229edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Myosuite task dataset paths\n",
    "dataset_root = Path(\"/juno/u/ksrini/multi_task_experts/datasets/myo10\")\n",
    "dataset_path = list(dataset_root.rglob(\"*.hdf5\"))[0]\n",
    "\n",
    "with h5py.File(dataset_path, \"r\") as data:\n",
    "    print([s.decode('utf-8') for s in data['mask']['traj_success']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2490fc",
   "metadata": {},
   "source": [
    "## Loading and running bidex task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import isaacgym, isaacgymenvs\n",
    "from omegaconf import OmegaConf\n",
    "from hydra import compose, initialize_config_dir\n",
    "from isaacgymenvs.utils.rlgames_utils import get_rlgames_env_creator\n",
    "from isaacgymenvs.utils.reformat import omegaconf_to_dict\n",
    "\n",
    "def get_expert_cfg(config_path=None, checkpoint_path=None):\n",
    "    if checkpoint_path:\n",
    "        config_path = os.path.join(os.path.dirname(os.path.dirname(checkpoint_path)), \"config.yaml\")\n",
    "    with open(config_path, \"r\") as f:\n",
    "        cfg = OmegaConf.load(config_path)\n",
    "    return cfg\n",
    "\n",
    "cfg_expert = get_expert_cfg(\"/mnt/ws-dmanip/isaacgymenvs/isaacgymenvs/runs/ArticulateSpray2Expert_12-20-34-27/config.yaml\")\n",
    "\n",
    "create_rlgpu_env = get_rlgames_env_creator(\n",
    "    cfg_expert['seed'],\n",
    "    omegaconf_to_dict(cfg_expert[\"task\"]),\n",
    "    cfg_expert[\"task_name\"],\n",
    "    cfg_expert[\"sim_device\"],\n",
    "    cfg_expert[\"rl_device\"],\n",
    "    cfg_expert['graphics_device_id'],\n",
    "    cfg_expert['headless'],\n",
    "    multi_gpu=cfg_expert['multi_gpu'],\n",
    "    )\n",
    "\n",
    "env = create_rlgpu_env()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.dirname(dataset_path)\n",
    "source_files = list(filter(lambda x: not x.startswith(\"merged\"), map(lambda x: os.path.join(dataset_dir, x), os.listdir(dataset_dir))))\n",
    "# merge all hdf5 files in dataset_path into a single hdf5 file\n",
    "merged_dataset_path = os.path.join(os.path.dirname(dataset_path), \"merged.hdf5\")\n",
    "if not os.path.exists(merged_dataset_path):\n",
    "    # os.remove(merged_dataset_path)\n",
    "    merge_hdf5_files(source_files, merged_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f9297",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(merged_dataset_path, \"r\") as f:\n",
    "    print(\"merged dataset has {} demonstrations\".format(len(f[\"data\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93687dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file\n",
    "f = h5py.File(merged_dataset_path, \"a\")\n",
    "\n",
    "# each demonstration is a group under \"data\"\n",
    "demos = list(f[\"data\"].keys())\n",
    "num_demos = len(demos)\n",
    "demo_grp = f[\"data\"][demos[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e896e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_keys = demo_grp[\"obs\"].keys()\n",
    "# demo_grp.create_group(\"next_obs\")\n",
    "for k in obs_keys:\n",
    "    demo_grp.create_dataset(f\"next_obs/{k}\", data=demo_grp[f\"obs/{k}\"][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57976238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the trajectory also contains the next observations under \"next_obs\", \n",
    "# for convenient use in a batch (offline) RL pipeline. Let's verify\n",
    "# that \"next_obs\" and \"obs\" are offset by 1.\n",
    "for k in demo_grp[\"obs\"]:\n",
    "    # obs_{t+1} == next_obs_{t}\n",
    "    assert(np.allclose(demo_grp[\"obs\"][k][1:], demo_grp[\"next_obs\"][k][:-1]))\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a8aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_articulate_to_robosuite(merged_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also have \"done\" and \"reward\" information stored in each trajectory.\n",
    "# In this case, we have sparse rewards that indicate task completion at\n",
    "# that timestep.\n",
    "dones = demo_grp[\"dones\"][:]\n",
    "rewards = demo_grp[\"rewards\"][:]\n",
    "print(\"dones\")\n",
    "print(dones)\n",
    "print(\"\")\n",
    "print(\"rewards\")\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360df27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each demonstration also contains metadata\n",
    "num_samples = demo_grp.attrs[\"num_samples\"] # number of samples in this trajectory\n",
    "mujoco_xml_file = demo_grp.attrs[\"model_file\"] # mujoco XML file for this demonstration\n",
    "print(mujoco_xml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f10f98f",
   "metadata": {},
   "source": [
    "Finally, let's take a look at some global metadata present in the file. The hdf5 file stores environment metadata which is a convenient way to understand which simulation environment (task) the dataset was collected on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b579caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_meta = json.loads(f[\"data\"].attrs[\"env_args\"])\n",
    "# note: we could also have used the following function:\n",
    "# env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path=dataset_path)\n",
    "print(\"==== Env Meta ====\")\n",
    "print(json.dumps(env_meta, indent=4))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395453a",
   "metadata": {},
   "source": [
    "## Visualizing demonstration trajectories\n",
    "\n",
    "Finally, let's play some of these demonstrations back in the simulation environment to easily visualize the data that was collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d613ab93",
   "metadata": {},
   "source": [
    "It turns out that the environment metadata stored in the hdf5 allows us to easily create a simulation environment that is consistent with the way the dataset was collected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import robomimic.utils.env_utils as EnvUtils\n",
    "\n",
    "# create simulation environment from environment metedata\n",
    "env = EnvUtils.create_env_from_metadata(\n",
    "    env_meta=env_meta, \n",
    "    render=False,            # no on-screen rendering\n",
    "    render_offscreen=True,   # off-screen rendering to support rendering video frames\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a47d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "\n",
    "# We normally need to make sure robomimic knows which observations are images (for the\n",
    "# data processing pipeline). This is usually inferred from your training config, but\n",
    "# since we are just playing back demonstrations, we just need to initialize robomimic\n",
    "# with a dummy spec.\n",
    "dummy_spec = dict(\n",
    "    obs=dict(\n",
    "            low_dim=[\"robot0_eef_pos\"],\n",
    "            rgb=[],\n",
    "        ),\n",
    ")\n",
    "ObsUtils.initialize_obs_utils_with_obs_specs(obs_modality_specs=dummy_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "# prepare to write playback trajectories to video\n",
    "video_path = os.path.join(download_folder, \"playback.mp4\")\n",
    "video_writer = imageio.get_writer(video_path, fps=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dabd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playback_recorded_trajectory(demo_key, camera_key=\"fixed_camera\"):\n",
    "    \"\"\"\n",
    "    Simple helper function to playback the trajectory stored under the hdf5 group @demo_key and\n",
    "    write frames rendered from the simulation to the active @video_writer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # robosuite datasets store the ground-truth simulator states under the \"states\" key.\n",
    "    # We will use the first one, alone with the model xml, to reset the environment to\n",
    "    # the initial configuration before playing back actions.\n",
    "    images = f[f\"data/{demo_key}/obs/{camera_key}\"][:]\n",
    "    for video_image in images:\n",
    "        video_writer.append_data(video_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playback_trajectory(demo_key):\n",
    "    \"\"\"\n",
    "    Simple helper function to playback the trajectory stored under the hdf5 group @demo_key and\n",
    "    write frames rendered from the simulation to the active @video_writer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # robosuite datasets store the ground-truth simulator states under the \"states\" key.\n",
    "    # We will use the first one, alone with the model xml, to reset the environment to\n",
    "    # the initial configuration before playing back actions.\n",
    "    init_state = f[\"data/{}/states\".format(demo_key)][0]\n",
    "    model_xml = f[\"data/{}\".format(demo_key)].attrs[\"model_file\"]\n",
    "    initial_state_dict = dict(states=init_state, model=model_xml)\n",
    "    \n",
    "    # reset to initial state\n",
    "    env.reset_to(initial_state_dict)\n",
    "    \n",
    "    # playback actions one by one, and render frames\n",
    "    actions = f[\"data/{}/actions\".format(demo_key)][:]\n",
    "    for t in range(actions.shape[0]):\n",
    "        env.step(actions[t])\n",
    "        video_img = env.render(mode=\"rgb_array\", height=512, width=512, camera_name=\"agentview\")\n",
    "        video_writer.append_data(video_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# playback the first 5 demos\n",
    "for ep in demos[:5]:\n",
    "    print(\"Playing back demo key: {}\".format(ep))\n",
    "    playback_recorded_trajectory(ep)\n",
    "\n",
    "# done writing video\n",
    "video_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the trajectories!\n",
    "from IPython.display import Video\n",
    "Video(video_path, embed=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
